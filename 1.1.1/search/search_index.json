{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Architecture Overview Introduction Usage Scenarios Architecture Features Introduction Apache Spark is a unified analytics engine for large-scale data processing, and Spark SQL is a Spark module widely used to process structured data in data center. However, Spark SQL still faces the challenge of stability and performance in a highly dynamic environment with ultra-large-scale data. SQL Index and Data Source Cache are designed to leverage users' defined indices and smart fine-grained in-memory data caching to boost Spark SQL performance, and can address the performance issues of some use cases. Usage Scenarios Usage Scenario 1 -- Interactive queries Most customers adopt Spark SQL as a batch processing engine. Unfortunately, customers would find it hard to separate batch processing and interactive use cases. Interactive queries need to return the data in seconds or even sub-seconds instead of the minutes or hours of batch processing. This is a big challenge for the current Spark SQL implementation. For example, the following interactive query attempts to filter out a very small result set from a huge fact table. select ss_sold_date_sk, ss_sold_time_sk, ss_item_sk, ss_cdemo_sk, ss_store_sk, ss_ticket_number, ss_ext_discount_amt, ss_ext_wholesale_cost, ss_ext_tax from fact.ss_sales where (date='20200801' and ss_customer='xxx' and ss_item_sk='806486\u2019) limit 10 Interactive queries usually process a large data set but return a small portion of data filtering for a specific condition. By creating and storing a full B+ Tree index for key columns and using a smart fine-grained in-memory data caching strategy, we can boost Spark SQL interactive queries to seconds and even sub-seconds. By properly using index and cache, the performance of some interactive queries can possibly be improved by order of magnitude. Usage Scenario 2 -- Batch processing jobs Customers usually use Spark SQL for Business Analytics in Data Warehousing. SQL Data Source Cache can speed up batch processing jobs with 2 cache strategies: Automatically cache hot data. Specifically cache hot tables. Users can choose either strategy based on their need. Architecture The following diagram shows the design architecture. SQL Index and Data Source Cache acts as a .jar plug-in for Spark SQL. We designed the compatible adapter layer for three columnar storage file formats: Parquet ORC oap(Parquet-like file format defined by OAP). SQL Index and Data Source Cache have a Unified Cache Representation for different columnar storage formats and a fine-grained cache unit for one column of a RowGroup. 2 major optimization functions (indexing and caching) are based on unified representation and the adapter. Indices can be created on one or multiple columns of a data file. Data Source Cache can cache both decompressed and decoded vectorized data and binary raw data. Generally, the server's DRAM is used as the cache medium. PMem can also be used as the cache medium as it will provide a more cost effective solution for the requirements of a high performance environment. Both indexing and caching as Optimizer & Execution are transparent for users. See the Features section for details. Spark ThriftServer * is a good use case for OAP, because ThriftServer launches Spark Applications which can cache hot data for a long time in the background, and it also accepts query requests from different clients at the same time. bin/spark-sql , bin/spark-shell or bin/pyspark can certainly be used by SQL Index and Data Source Cache, but usually only for interactive test situations. Features Use indexing and caching to improve Spark SQL performance on ad-hoc queries and batch processing jobs. Indexing Users can use SQL DDL(create/drop/refresh/check/show index) to use indexing. Once users create indices using DDL, index files are generated in a specific directory and mainly composed of index data and statistics. When queries are executed, analyzing index files to boost performance is transparent to users. BTREE, BITMAP Index is an optimization that is widely used in traditional databases. We also adopt these two index types in the project. BTREE indexing is intended for datasets that have a lot of distinct values, and are distributed randomly, such as telephone numbers or ID numbers. BitMap index is intended for datasets with a limited total amount of distinct values, such as state or age. Statistics are located in the index file after all the index data are written into the index file. Sometimes, reading indices could bring extra cost for some queries. So we also support 4 statistics (MinMax, Bloom Filter, SampleBase and PartByValue) to help filter. With statistics, we can make sure we only use indices if we can possibly improve the execution. Caching Caching is another core feature of OAP. It is also transparent to users. Data Source Cache can automatically load frequently queried (hot) data, and evict data automatically according to the LRU policy when cache is full. Data Source Cache has the following characteristics: Off-Heap memory. The Data Source Cache uses off-heap memory and avoids the JVM GC. It can also use PMem as high-performance, high-capacity, low-cost memory Cache-Locality. Data Source Cache can schedule computing tasks to the executor which holds needed data in cache, by implementing a cache aware mechanism based on Spark driver and executors communication. Cache granularity. A column in one RowGroup (equivalent to Stripe in ORC) of a column-oriented storage format file is loaded into a basic cache unit which is called a \"Fiber\" in OAP. Cache Eviction. Data Source Cache cache eviction uses LRU policy, and automatically caches and evicts data transparently to end user. Cache configured tables. Data Source Cache also supports caching specific tables by configuring items according to actual situations, these tables are usually hot tables. *Other names and brands may be claimed as the property of others.","title":"Architecture Overview"},{"location":"#architecture-overview","text":"Introduction Usage Scenarios Architecture Features","title":"Architecture Overview"},{"location":"#introduction","text":"Apache Spark is a unified analytics engine for large-scale data processing, and Spark SQL is a Spark module widely used to process structured data in data center. However, Spark SQL still faces the challenge of stability and performance in a highly dynamic environment with ultra-large-scale data. SQL Index and Data Source Cache are designed to leverage users' defined indices and smart fine-grained in-memory data caching to boost Spark SQL performance, and can address the performance issues of some use cases.","title":"Introduction"},{"location":"#usage-scenarios","text":"","title":"Usage Scenarios"},{"location":"#usage-scenario-1-interactive-queries","text":"Most customers adopt Spark SQL as a batch processing engine. Unfortunately, customers would find it hard to separate batch processing and interactive use cases. Interactive queries need to return the data in seconds or even sub-seconds instead of the minutes or hours of batch processing. This is a big challenge for the current Spark SQL implementation. For example, the following interactive query attempts to filter out a very small result set from a huge fact table. select ss_sold_date_sk, ss_sold_time_sk, ss_item_sk, ss_cdemo_sk, ss_store_sk, ss_ticket_number, ss_ext_discount_amt, ss_ext_wholesale_cost, ss_ext_tax from fact.ss_sales where (date='20200801' and ss_customer='xxx' and ss_item_sk='806486\u2019) limit 10 Interactive queries usually process a large data set but return a small portion of data filtering for a specific condition. By creating and storing a full B+ Tree index for key columns and using a smart fine-grained in-memory data caching strategy, we can boost Spark SQL interactive queries to seconds and even sub-seconds. By properly using index and cache, the performance of some interactive queries can possibly be improved by order of magnitude.","title":"Usage Scenario 1 -- Interactive queries"},{"location":"#usage-scenario-2-batch-processing-jobs","text":"Customers usually use Spark SQL for Business Analytics in Data Warehousing. SQL Data Source Cache can speed up batch processing jobs with 2 cache strategies: Automatically cache hot data. Specifically cache hot tables. Users can choose either strategy based on their need.","title":"Usage Scenario 2 -- Batch processing jobs"},{"location":"#architecture","text":"The following diagram shows the design architecture. SQL Index and Data Source Cache acts as a .jar plug-in for Spark SQL. We designed the compatible adapter layer for three columnar storage file formats: Parquet ORC oap(Parquet-like file format defined by OAP). SQL Index and Data Source Cache have a Unified Cache Representation for different columnar storage formats and a fine-grained cache unit for one column of a RowGroup. 2 major optimization functions (indexing and caching) are based on unified representation and the adapter. Indices can be created on one or multiple columns of a data file. Data Source Cache can cache both decompressed and decoded vectorized data and binary raw data. Generally, the server's DRAM is used as the cache medium. PMem can also be used as the cache medium as it will provide a more cost effective solution for the requirements of a high performance environment. Both indexing and caching as Optimizer & Execution are transparent for users. See the Features section for details. Spark ThriftServer * is a good use case for OAP, because ThriftServer launches Spark Applications which can cache hot data for a long time in the background, and it also accepts query requests from different clients at the same time. bin/spark-sql , bin/spark-shell or bin/pyspark can certainly be used by SQL Index and Data Source Cache, but usually only for interactive test situations.","title":"Architecture"},{"location":"#features","text":"Use indexing and caching to improve Spark SQL performance on ad-hoc queries and batch processing jobs.","title":"Features"},{"location":"#indexing","text":"Users can use SQL DDL(create/drop/refresh/check/show index) to use indexing. Once users create indices using DDL, index files are generated in a specific directory and mainly composed of index data and statistics. When queries are executed, analyzing index files to boost performance is transparent to users. BTREE, BITMAP Index is an optimization that is widely used in traditional databases. We also adopt these two index types in the project. BTREE indexing is intended for datasets that have a lot of distinct values, and are distributed randomly, such as telephone numbers or ID numbers. BitMap index is intended for datasets with a limited total amount of distinct values, such as state or age. Statistics are located in the index file after all the index data are written into the index file. Sometimes, reading indices could bring extra cost for some queries. So we also support 4 statistics (MinMax, Bloom Filter, SampleBase and PartByValue) to help filter. With statistics, we can make sure we only use indices if we can possibly improve the execution.","title":"Indexing"},{"location":"#caching","text":"Caching is another core feature of OAP. It is also transparent to users. Data Source Cache can automatically load frequently queried (hot) data, and evict data automatically according to the LRU policy when cache is full. Data Source Cache has the following characteristics: Off-Heap memory. The Data Source Cache uses off-heap memory and avoids the JVM GC. It can also use PMem as high-performance, high-capacity, low-cost memory Cache-Locality. Data Source Cache can schedule computing tasks to the executor which holds needed data in cache, by implementing a cache aware mechanism based on Spark driver and executors communication. Cache granularity. A column in one RowGroup (equivalent to Stripe in ORC) of a column-oriented storage format file is loaded into a basic cache unit which is called a \"Fiber\" in OAP. Cache Eviction. Data Source Cache cache eviction uses LRU policy, and automatically caches and evicts data transparently to end user. Cache configured tables. Data Source Cache also supports caching specific tables by configuring items according to actual situations, these tables are usually hot tables.","title":"Caching"},{"location":"#other-names-and-brands-may-be-claimed-as-the-property-of-others","text":"","title":"*Other names and brands may be claimed as the property of others."},{"location":"Advanced-Configuration/","text":"Advanced Configuration In addition to usage information provided in User Guide , we provide more strategies for SQL Index and Data Source Cache in this section. Cache Hot Tables Data Source Cache also supports caching specific tables by configuring items according to actual situations, these tables are usually hot tables. To enable caching specific hot tables, you can add the configuration below to spark-defaults.conf . # enable table lists fiberCache spark.sql.oap.cache.table.list.enabled true # Table lists using fiberCache actively spark.sql.oap.cache.table.list <databasename>.<tablename1>;<databasename>.<tablename2> Column Vector Cache This document above use binary cache for Parquet as example, cause binary cache can improve cache space utilization compared to ColumnVector cache. When your cluster memory resources are abundant enough, you can choose ColumnVector cache to spare computation time. To enable ColumnVector data cache for Parquet file format, you should add the configuration below to spark-defaults.conf . # for parquet file format, disable binary cache spark.sql.oap.parquet.binary.cache.enabled false # for parquet file format, enable ColumnVector cache spark.sql.oap.parquet.data.cache.enabled true Large Scale and Heterogeneous Cluster Support NOTE: Only works with external cache OAP influences Spark to schedule tasks according to cache locality info. This info could be of large amount in a large scale cluster , and how to schedule tasks in a heterogeneous cluster (some nodes with PMem, some without) could also be challenging. We introduce an external DB to store cache locality info. If there's no cache available, Spark will fall back to schedule respecting HDFS locality. Currently we support Redis as external DB service. Please download and launch a redis-server before running Spark with OAP. Please add the following configurations to spark-defaults.conf . spark.sql.oap.external.cache.metaDB.enabled true # Redis-server address spark.sql.oap.external.cache.metaDB.address 10.1.2.12 spark.sql.oap.external.cache.metaDB.impl org.apache.spark.sql.execution.datasources.RedisClient","title":"Advanced Configuration"},{"location":"Advanced-Configuration/#advanced-configuration","text":"In addition to usage information provided in User Guide , we provide more strategies for SQL Index and Data Source Cache in this section.","title":"Advanced Configuration"},{"location":"Advanced-Configuration/#cache-hot-tables","text":"Data Source Cache also supports caching specific tables by configuring items according to actual situations, these tables are usually hot tables. To enable caching specific hot tables, you can add the configuration below to spark-defaults.conf . # enable table lists fiberCache spark.sql.oap.cache.table.list.enabled true # Table lists using fiberCache actively spark.sql.oap.cache.table.list <databasename>.<tablename1>;<databasename>.<tablename2>","title":"Cache Hot Tables"},{"location":"Advanced-Configuration/#column-vector-cache","text":"This document above use binary cache for Parquet as example, cause binary cache can improve cache space utilization compared to ColumnVector cache. When your cluster memory resources are abundant enough, you can choose ColumnVector cache to spare computation time. To enable ColumnVector data cache for Parquet file format, you should add the configuration below to spark-defaults.conf . # for parquet file format, disable binary cache spark.sql.oap.parquet.binary.cache.enabled false # for parquet file format, enable ColumnVector cache spark.sql.oap.parquet.data.cache.enabled true","title":"Column Vector Cache"},{"location":"Advanced-Configuration/#large-scale-and-heterogeneous-cluster-support","text":"NOTE: Only works with external cache OAP influences Spark to schedule tasks according to cache locality info. This info could be of large amount in a large scale cluster , and how to schedule tasks in a heterogeneous cluster (some nodes with PMem, some without) could also be challenging. We introduce an external DB to store cache locality info. If there's no cache available, Spark will fall back to schedule respecting HDFS locality. Currently we support Redis as external DB service. Please download and launch a redis-server before running Spark with OAP. Please add the following configurations to spark-defaults.conf . spark.sql.oap.external.cache.metaDB.enabled true # Redis-server address spark.sql.oap.external.cache.metaDB.address 10.1.2.12 spark.sql.oap.external.cache.metaDB.impl org.apache.spark.sql.execution.datasources.RedisClient","title":"Large Scale and Heterogeneous Cluster Support"},{"location":"Architect-Overview/","text":"Architecture Overview Introduction Usage Scenarios Architecture Features Introduction Apache Spark is a unified analytics engine for large-scale data processing, and Spark SQL is a Spark module widely used to process structured data in data center. However, Spark SQL still faces the challenge of stability and performance in a highly dynamic environment with ultra-large-scale data. SQL Index and Data Source Cache are designed to leverage users' defined indices and smart fine-grained in-memory data caching to boost Spark SQL performance, and can address the performance issues of some use cases. Usage Scenarios Usage Scenario 1 -- Interactive queries Most customers adopt Spark SQL as a batch processing engine. Unfortunately, customers would find it hard to separate batch processing and interactive use cases. Interactive queries need to return the data in seconds or even sub-seconds instead of the minutes or hours of batch processing. This is a big challenge for the current Spark SQL implementation. For example, the following interactive query attempts to filter out a very small result set from a huge fact table. select ss_sold_date_sk, ss_sold_time_sk, ss_item_sk, ss_cdemo_sk, ss_store_sk, ss_ticket_number, ss_ext_discount_amt, ss_ext_wholesale_cost, ss_ext_tax from fact.ss_sales where (date='20200801' and ss_customer='xxx' and ss_item_sk='806486\u2019) limit 10 Interactive queries usually process a large data set but return a small portion of data filtering for a specific condition. By creating and storing a full B+ Tree index for key columns and using a smart fine-grained in-memory data caching strategy, we can boost Spark SQL interactive queries to seconds and even sub-seconds. By properly using index and cache, the performance of some interactive queries can possibly be improved by order of magnitude. Usage Scenario 2 -- Batch processing jobs Customers usually use Spark SQL for Business Analytics in Data Warehousing. SQL Data Source Cache can speed up batch processing jobs with 2 cache strategies: Automatically cache hot data. Specifically cache hot tables. Users can choose either strategy based on their need. Architecture The following diagram shows the design architecture. SQL Index and Data Source Cache acts as a .jar plug-in for Spark SQL. We designed the compatible adapter layer for three columnar storage file formats: Parquet ORC oap(Parquet-like file format defined by OAP). SQL Index and Data Source Cache have a Unified Cache Representation for different columnar storage formats and a fine-grained cache unit for one column of a RowGroup. 2 major optimization functions (indexing and caching) are based on unified representation and the adapter. Indices can be created on one or multiple columns of a data file. Data Source Cache can cache both decompressed and decoded vectorized data and binary raw data. Generally, the server's DRAM is used as the cache medium. PMem can also be used as the cache medium as it will provide a more cost effective solution for the requirements of a high performance environment. Both indexing and caching as Optimizer & Execution are transparent for users. See the Features section for details. Spark ThriftServer * is a good use case for OAP, because ThriftServer launches Spark Applications which can cache hot data for a long time in the background, and it also accepts query requests from different clients at the same time. bin/spark-sql , bin/spark-shell or bin/pyspark can certainly be used by SQL Index and Data Source Cache, but usually only for interactive test situations. Features Use indexing and caching to improve Spark SQL performance on ad-hoc queries and batch processing jobs. Indexing Users can use SQL DDL(create/drop/refresh/check/show index) to use indexing. Once users create indices using DDL, index files are generated in a specific directory and mainly composed of index data and statistics. When queries are executed, analyzing index files to boost performance is transparent to users. BTREE, BITMAP Index is an optimization that is widely used in traditional databases. We also adopt these two index types in the project. BTREE indexing is intended for datasets that have a lot of distinct values, and are distributed randomly, such as telephone numbers or ID numbers. BitMap index is intended for datasets with a limited total amount of distinct values, such as state or age. Statistics are located in the index file after all the index data are written into the index file. Sometimes, reading indices could bring extra cost for some queries. So we also support 4 statistics (MinMax, Bloom Filter, SampleBase and PartByValue) to help filter. With statistics, we can make sure we only use indices if we can possibly improve the execution. Caching Caching is another core feature of OAP. It is also transparent to users. Data Source Cache can automatically load frequently queried (hot) data, and evict data automatically according to the LRU policy when cache is full. Data Source Cache has the following characteristics: Off-Heap memory. The Data Source Cache uses off-heap memory and avoids the JVM GC. It can also use PMem as high-performance, high-capacity, low-cost memory Cache-Locality. Data Source Cache can schedule computing tasks to the executor which holds needed data in cache, by implementing a cache aware mechanism based on Spark driver and executors communication. Cache granularity. A column in one RowGroup (equivalent to Stripe in ORC) of a column-oriented storage format file is loaded into a basic cache unit which is called a \"Fiber\" in OAP. Cache Eviction. Data Source Cache cache eviction uses LRU policy, and automatically caches and evicts data transparently to end user. Cache configured tables. Data Source Cache also supports caching specific tables by configuring items according to actual situations, these tables are usually hot tables. *Other names and brands may be claimed as the property of others.","title":"Architecture Overview"},{"location":"Architect-Overview/#architecture-overview","text":"Introduction Usage Scenarios Architecture Features","title":"Architecture Overview"},{"location":"Architect-Overview/#introduction","text":"Apache Spark is a unified analytics engine for large-scale data processing, and Spark SQL is a Spark module widely used to process structured data in data center. However, Spark SQL still faces the challenge of stability and performance in a highly dynamic environment with ultra-large-scale data. SQL Index and Data Source Cache are designed to leverage users' defined indices and smart fine-grained in-memory data caching to boost Spark SQL performance, and can address the performance issues of some use cases.","title":"Introduction"},{"location":"Architect-Overview/#usage-scenarios","text":"","title":"Usage Scenarios"},{"location":"Architect-Overview/#usage-scenario-1-interactive-queries","text":"Most customers adopt Spark SQL as a batch processing engine. Unfortunately, customers would find it hard to separate batch processing and interactive use cases. Interactive queries need to return the data in seconds or even sub-seconds instead of the minutes or hours of batch processing. This is a big challenge for the current Spark SQL implementation. For example, the following interactive query attempts to filter out a very small result set from a huge fact table. select ss_sold_date_sk, ss_sold_time_sk, ss_item_sk, ss_cdemo_sk, ss_store_sk, ss_ticket_number, ss_ext_discount_amt, ss_ext_wholesale_cost, ss_ext_tax from fact.ss_sales where (date='20200801' and ss_customer='xxx' and ss_item_sk='806486\u2019) limit 10 Interactive queries usually process a large data set but return a small portion of data filtering for a specific condition. By creating and storing a full B+ Tree index for key columns and using a smart fine-grained in-memory data caching strategy, we can boost Spark SQL interactive queries to seconds and even sub-seconds. By properly using index and cache, the performance of some interactive queries can possibly be improved by order of magnitude.","title":"Usage Scenario 1 -- Interactive queries"},{"location":"Architect-Overview/#usage-scenario-2-batch-processing-jobs","text":"Customers usually use Spark SQL for Business Analytics in Data Warehousing. SQL Data Source Cache can speed up batch processing jobs with 2 cache strategies: Automatically cache hot data. Specifically cache hot tables. Users can choose either strategy based on their need.","title":"Usage Scenario 2 -- Batch processing jobs"},{"location":"Architect-Overview/#architecture","text":"The following diagram shows the design architecture. SQL Index and Data Source Cache acts as a .jar plug-in for Spark SQL. We designed the compatible adapter layer for three columnar storage file formats: Parquet ORC oap(Parquet-like file format defined by OAP). SQL Index and Data Source Cache have a Unified Cache Representation for different columnar storage formats and a fine-grained cache unit for one column of a RowGroup. 2 major optimization functions (indexing and caching) are based on unified representation and the adapter. Indices can be created on one or multiple columns of a data file. Data Source Cache can cache both decompressed and decoded vectorized data and binary raw data. Generally, the server's DRAM is used as the cache medium. PMem can also be used as the cache medium as it will provide a more cost effective solution for the requirements of a high performance environment. Both indexing and caching as Optimizer & Execution are transparent for users. See the Features section for details. Spark ThriftServer * is a good use case for OAP, because ThriftServer launches Spark Applications which can cache hot data for a long time in the background, and it also accepts query requests from different clients at the same time. bin/spark-sql , bin/spark-shell or bin/pyspark can certainly be used by SQL Index and Data Source Cache, but usually only for interactive test situations.","title":"Architecture"},{"location":"Architect-Overview/#features","text":"Use indexing and caching to improve Spark SQL performance on ad-hoc queries and batch processing jobs.","title":"Features"},{"location":"Architect-Overview/#indexing","text":"Users can use SQL DDL(create/drop/refresh/check/show index) to use indexing. Once users create indices using DDL, index files are generated in a specific directory and mainly composed of index data and statistics. When queries are executed, analyzing index files to boost performance is transparent to users. BTREE, BITMAP Index is an optimization that is widely used in traditional databases. We also adopt these two index types in the project. BTREE indexing is intended for datasets that have a lot of distinct values, and are distributed randomly, such as telephone numbers or ID numbers. BitMap index is intended for datasets with a limited total amount of distinct values, such as state or age. Statistics are located in the index file after all the index data are written into the index file. Sometimes, reading indices could bring extra cost for some queries. So we also support 4 statistics (MinMax, Bloom Filter, SampleBase and PartByValue) to help filter. With statistics, we can make sure we only use indices if we can possibly improve the execution.","title":"Indexing"},{"location":"Architect-Overview/#caching","text":"Caching is another core feature of OAP. It is also transparent to users. Data Source Cache can automatically load frequently queried (hot) data, and evict data automatically according to the LRU policy when cache is full. Data Source Cache has the following characteristics: Off-Heap memory. The Data Source Cache uses off-heap memory and avoids the JVM GC. It can also use PMem as high-performance, high-capacity, low-cost memory Cache-Locality. Data Source Cache can schedule computing tasks to the executor which holds needed data in cache, by implementing a cache aware mechanism based on Spark driver and executors communication. Cache granularity. A column in one RowGroup (equivalent to Stripe in ORC) of a column-oriented storage format file is loaded into a basic cache unit which is called a \"Fiber\" in OAP. Cache Eviction. Data Source Cache cache eviction uses LRU policy, and automatically caches and evicts data transparently to end user. Cache configured tables. Data Source Cache also supports caching specific tables by configuring items according to actual situations, these tables are usually hot tables.","title":"Caching"},{"location":"Architect-Overview/#other-names-and-brands-may-be-claimed-as-the-property-of-others","text":"","title":"*Other names and brands may be claimed as the property of others."},{"location":"Developer-Guide/","text":"Developer Guide This document is a supplement to the whole OAP Developer Guide for SQL Index and Data Source Cache. After following that document, you can continue more details for SQL Index and Data Source Cache. Building Prerequisites for building Building with Apache Maven* . Before building, install PMem-Common locally: git clone -b <tag-version> https://github.com/oap-project/pmem-common.git cd pmem-common mvn clean install -DskipTests Install the required packages on the build system: cmake Plasma Plasma installation To use optimized Plasma cache with OAP, you need following components: (1) libarrow.so , libplasma.so , libplasma_java.so : dynamic libraries, will be used in Plasma client. (2) plasma-store-server : executable file, Plasma cache service. (3) arrow-plasma-4.0.0.jar : will be used when compile oap and spark runtime also need it. .so file and binary file Clone code from Arrow repo and run following commands, this will install libplasma.so , libarrow.so , libplasma_java.so and plasma-store-server to your system path( /usr/lib64 by default). And if you are using Spark in a cluster environment, you can copy these files to all nodes in your cluster if the OS or distribution are same, otherwise, you need compile it on each node. cd /tmp git clone https://github.com/oap-project/arrow.git cd arrow && git checkout arrow-4.0.0-oap-1.1.1 cd cpp mkdir release cd release #build libarrow, libplasma, libplasma_java cmake -DCMAKE_INSTALL_PREFIX=/usr/ -DCMAKE_BUILD_TYPE=Release -DARROW_BUILD_TESTS=on -DARROW_PLASMA_JAVA_CLIENT=on -DARROW_PLASMA=on -DARROW_DEPENDENCY_SOURCE=BUNDLED .. make -j$(nproc) sudo make install -j$(nproc) arrow-plasma-4.0.0.jar Run following command, this will install arrow jars to your local maven repo. Besides, you need copy arrow-plasma-4.0.0.jar to $SPARK_HOME/jars/ dir, cause this jar is needed when using external cache. cd /tmp/arrow/java mvn clean -q -pl plasma -am -DskipTests install Build the SQL DS Cache package: git clone -b <tag-version> https://github.com/oap-project/sql-ds-cache.git cd sql-ds-cache mvn clean -DskipTests package Running Tests Run all the tests: mvn clean test Run a specific test suite, for example OapDDLSuite : mvn -DwildcardSuites=org.apache.spark.sql.execution.datasources.oap.OapDDLSuite test NOTE : Log level of unit tests currently default to ERROR, please override sql-ds-cache/Plasma-based-cache/src/test/resources/log4j.properties if needed. *Other names and brands may be claimed as the property of others.","title":"Developer Guide"},{"location":"Developer-Guide/#developer-guide","text":"This document is a supplement to the whole OAP Developer Guide for SQL Index and Data Source Cache. After following that document, you can continue more details for SQL Index and Data Source Cache.","title":"Developer Guide"},{"location":"Developer-Guide/#building","text":"","title":"Building"},{"location":"Developer-Guide/#prerequisites-for-building","text":"Building with Apache Maven* . Before building, install PMem-Common locally: git clone -b <tag-version> https://github.com/oap-project/pmem-common.git cd pmem-common mvn clean install -DskipTests Install the required packages on the build system: cmake Plasma","title":"Prerequisites for building"},{"location":"Developer-Guide/#plasma-installation","text":"To use optimized Plasma cache with OAP, you need following components: (1) libarrow.so , libplasma.so , libplasma_java.so : dynamic libraries, will be used in Plasma client. (2) plasma-store-server : executable file, Plasma cache service. (3) arrow-plasma-4.0.0.jar : will be used when compile oap and spark runtime also need it. .so file and binary file Clone code from Arrow repo and run following commands, this will install libplasma.so , libarrow.so , libplasma_java.so and plasma-store-server to your system path( /usr/lib64 by default). And if you are using Spark in a cluster environment, you can copy these files to all nodes in your cluster if the OS or distribution are same, otherwise, you need compile it on each node. cd /tmp git clone https://github.com/oap-project/arrow.git cd arrow && git checkout arrow-4.0.0-oap-1.1.1 cd cpp mkdir release cd release #build libarrow, libplasma, libplasma_java cmake -DCMAKE_INSTALL_PREFIX=/usr/ -DCMAKE_BUILD_TYPE=Release -DARROW_BUILD_TESTS=on -DARROW_PLASMA_JAVA_CLIENT=on -DARROW_PLASMA=on -DARROW_DEPENDENCY_SOURCE=BUNDLED .. make -j$(nproc) sudo make install -j$(nproc) arrow-plasma-4.0.0.jar Run following command, this will install arrow jars to your local maven repo. Besides, you need copy arrow-plasma-4.0.0.jar to $SPARK_HOME/jars/ dir, cause this jar is needed when using external cache. cd /tmp/arrow/java mvn clean -q -pl plasma -am -DskipTests install Build the SQL DS Cache package: git clone -b <tag-version> https://github.com/oap-project/sql-ds-cache.git cd sql-ds-cache mvn clean -DskipTests package","title":"Plasma installation"},{"location":"Developer-Guide/#running-tests","text":"Run all the tests: mvn clean test Run a specific test suite, for example OapDDLSuite : mvn -DwildcardSuites=org.apache.spark.sql.execution.datasources.oap.OapDDLSuite test NOTE : Log level of unit tests currently default to ERROR, please override sql-ds-cache/Plasma-based-cache/src/test/resources/log4j.properties if needed.","title":"Running Tests"},{"location":"Developer-Guide/#other-names-and-brands-may-be-claimed-as-the-property-of-others","text":"","title":"*Other names and brands may be claimed as the property of others."},{"location":"HCFS-User-Guide/","text":"HCFS User Guide Prerequisites Configurations Prerequisites HCFS based Data Source Cache on Spark 3.1.1 requires a working Hadoop cluster with YARN and Spark. Running Spark on YARN requires a binary distribution of Spark, which is built with YARN support. The HCFS based Data Source Cache also need to install plasma and redis, please follow OAP-Installation-Guide for how to install plasma and redis. Configurations Spark Configurations Before you run $SPARK_HOME/bin/spark-shell , you need to configure Spark for integration. You need to add or update the following configurations in the Spark configuration file $SPARK_HOME/conf/spark-defaults.conf on your working node. spark.hadoop.fs.cachedFs.impl com.intel.oap.fs.hadoop.cachedfs.CachedFileSystem # absolute path of the jar on your working node spark.files /path/to/hcfs-sql-ds-cache-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.executor.extraClassPath ./hcfs-sql-cache-<version>.jar # absolute path of the jar on your working node spark.driver.extraClassPath /path/to/hcfs-sql-ds-cache-<version>.jar Redis Configuration Add the following configuration to $SPARK_HOME/conf/spark-defaults.conf . spark.hadoop.fs.cachedFs.redis.host $HOST spark.hadoop.fs.cachedFs.redis.port $PORT Configuration for HCFS cache location policy We provide three HCFS cache location policies, you can choose the best one for you workload * default policy This policy the file block locations consist of cached blocks and hdfs blocks (if cached blocks are incomplete) * cache_over_hdfs This policy use cached block location only if all requested content is cached, otherwise use HDFS block locations * hdfs_only This policy will ignoring cached blocks when finding file block locations Add the following configuration to $SPARK_HOME/conf/spark-defaults.conf . spark.hadoop.fs.cachedFs.blockLocation.policy default or cache_over_hdfs or hdfs_only Configuration for HCFS cache path pattern We provide HCFS cache patterns for paths to determine wherthe path will be cached * allowlist The path match the pattens will be cached. An empty regexp results in matching everything. eg. cachedFs://localhost:9000/dir/ * denylist The path match the pattens will not be cached. An empty regexp results in no matching of deny list. eg. io_data|io_control Add the following configuration to $SPARK_HOME/conf/spark-defaults.conf . spark.hadoop.fs.cachedFs.allowList.regexp $PATTEN spark.hadoop.fs.cachedFs.denylist.regexp $PATTERN","title":"HCFS User Guide"},{"location":"HCFS-User-Guide/#hcfs-user-guide","text":"Prerequisites Configurations","title":"HCFS User Guide"},{"location":"HCFS-User-Guide/#prerequisites","text":"HCFS based Data Source Cache on Spark 3.1.1 requires a working Hadoop cluster with YARN and Spark. Running Spark on YARN requires a binary distribution of Spark, which is built with YARN support. The HCFS based Data Source Cache also need to install plasma and redis, please follow OAP-Installation-Guide for how to install plasma and redis.","title":"Prerequisites"},{"location":"HCFS-User-Guide/#configurations","text":"","title":"Configurations"},{"location":"HCFS-User-Guide/#spark-configurations","text":"Before you run $SPARK_HOME/bin/spark-shell , you need to configure Spark for integration. You need to add or update the following configurations in the Spark configuration file $SPARK_HOME/conf/spark-defaults.conf on your working node. spark.hadoop.fs.cachedFs.impl com.intel.oap.fs.hadoop.cachedfs.CachedFileSystem # absolute path of the jar on your working node spark.files /path/to/hcfs-sql-ds-cache-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.executor.extraClassPath ./hcfs-sql-cache-<version>.jar # absolute path of the jar on your working node spark.driver.extraClassPath /path/to/hcfs-sql-ds-cache-<version>.jar","title":"Spark Configurations"},{"location":"HCFS-User-Guide/#redis-configuration","text":"Add the following configuration to $SPARK_HOME/conf/spark-defaults.conf . spark.hadoop.fs.cachedFs.redis.host $HOST spark.hadoop.fs.cachedFs.redis.port $PORT","title":"Redis Configuration"},{"location":"HCFS-User-Guide/#configuration-for-hcfs-cache-location-policy","text":"We provide three HCFS cache location policies, you can choose the best one for you workload * default policy This policy the file block locations consist of cached blocks and hdfs blocks (if cached blocks are incomplete) * cache_over_hdfs This policy use cached block location only if all requested content is cached, otherwise use HDFS block locations * hdfs_only This policy will ignoring cached blocks when finding file block locations Add the following configuration to $SPARK_HOME/conf/spark-defaults.conf . spark.hadoop.fs.cachedFs.blockLocation.policy default or cache_over_hdfs or hdfs_only","title":"Configuration for HCFS cache location policy"},{"location":"HCFS-User-Guide/#configuration-for-hcfs-cache-path-pattern","text":"We provide HCFS cache patterns for paths to determine wherthe path will be cached * allowlist The path match the pattens will be cached. An empty regexp results in matching everything. eg. cachedFs://localhost:9000/dir/ * denylist The path match the pattens will not be cached. An empty regexp results in no matching of deny list. eg. io_data|io_control Add the following configuration to $SPARK_HOME/conf/spark-defaults.conf . spark.hadoop.fs.cachedFs.allowList.regexp $PATTEN spark.hadoop.fs.cachedFs.denylist.regexp $PATTERN","title":"Configuration for HCFS cache path pattern"},{"location":"OAP-Developer-Guide/","text":"OAP Developer Guide This document contains the instructions & scripts on installing necessary dependencies and building OAP modules. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Native SQL Engine Building OAP Prerequisites We provide scripts to help automatically install dependencies required, please change to root user and run: # git clone -b <tag-version> https://github.com/oap-project/oap-tools.git # cd oap-tools # sh dev/install-compile-time-dependencies.sh Note : oap-tools tag version v1.1.1-spark-3.1.1 corresponds to all OAP modules' tag version v1.1.1-spark-3.1.1 . Then the dependencies below will be installed: Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow LLVM Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance. Building Building OAP OAP is built with Apache Maven and Oracle Java 8. To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package , which contains all OAP module jars. Change to root user, run # cd oap-tools # sh dev/compile-oap.sh Building OAP specific module If you just want to build a specific OAP Module, such as sql-ds-cache , change to root user, then run: # cd oap-tools # sh dev/compile-oap.sh --sql-ds-cache","title":"OAP Developer Guide"},{"location":"OAP-Developer-Guide/#oap-developer-guide","text":"This document contains the instructions & scripts on installing necessary dependencies and building OAP modules. You can get more detailed information from OAP each module below. SQL Index and Data Source Cache PMem Common PMem Spill PMem Shuffle Remote Shuffle OAP MLlib Native SQL Engine","title":"OAP Developer Guide"},{"location":"OAP-Developer-Guide/#building-oap","text":"","title":"Building OAP"},{"location":"OAP-Developer-Guide/#prerequisites","text":"We provide scripts to help automatically install dependencies required, please change to root user and run: # git clone -b <tag-version> https://github.com/oap-project/oap-tools.git # cd oap-tools # sh dev/install-compile-time-dependencies.sh Note : oap-tools tag version v1.1.1-spark-3.1.1 corresponds to all OAP modules' tag version v1.1.1-spark-3.1.1 . Then the dependencies below will be installed: Cmake GCC > 7 Memkind Vmemcache HPNL PMDK OneAPI Arrow LLVM Requirements for Shuffle Remote PMem Extension If enable Shuffle Remote PMem extension with RDMA, you can refer to PMem Shuffle to configure and validate RDMA in advance.","title":"Prerequisites"},{"location":"OAP-Developer-Guide/#building","text":"","title":"Building"},{"location":"OAP-Developer-Guide/#building-oap_1","text":"OAP is built with Apache Maven and Oracle Java 8. To build OAP package, run command below then you can find a tarball named oap-$VERSION-bin-spark-$VERSION.tar.gz under directory $OAP_TOOLS_HOME/dev/release-package , which contains all OAP module jars. Change to root user, run # cd oap-tools # sh dev/compile-oap.sh","title":"Building OAP"},{"location":"OAP-Developer-Guide/#building-oap-specific-module","text":"If you just want to build a specific OAP Module, such as sql-ds-cache , change to root user, then run: # cd oap-tools # sh dev/compile-oap.sh --sql-ds-cache","title":"Building OAP specific module"},{"location":"OAP-Installation-Guide/","text":"OAP Installation Guide This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine. Contents Prerequisites Installing OAP Configuration Prerequisites OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly. Installing OAP Create a Conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.1.1 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI Extra Steps for Shuffle Remote PMem Extension If you use one of OAP features -- PMem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details. Configuration Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar Then you can follow the corresponding feature documents for more details to use them.","title":"OAP Installation Guide"},{"location":"OAP-Installation-Guide/#oap-installation-guide","text":"This document introduces how to install OAP and its dependencies on your cluster nodes by Conda . Follow steps below on every node of your cluster to set right environment for each machine.","title":"OAP Installation Guide"},{"location":"OAP-Installation-Guide/#contents","text":"Prerequisites Installing OAP Configuration","title":"Contents"},{"location":"OAP-Installation-Guide/#prerequisites","text":"OS Requirements We have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use Fedora 29 CentOS 7.6 or above . Besides, for Memkind we recommend you use kernel above 3.10 . Conda Requirements Install Conda on your cluster nodes with below commands and follow the prompts on the installer screens.: $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh $ chmod +x Miniconda2-latest-Linux-x86_64.sh $ bash Miniconda2-latest-Linux-x86_64.sh For changes to take effect, close and re-open your current shell. To test your installation, run the command conda list in your terminal window. A list of installed packages appears if it has been installed correctly.","title":"Prerequisites"},{"location":"OAP-Installation-Guide/#installing-oap","text":"Create a Conda environment and install OAP Conda package. $ conda create -n oapenv -y python=3.7 $ conda activate oapenv $ conda install -c conda-forge -c intel -y oap=1.1.1 Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps. Arrow Plasma Memkind Vmemcache HPNL PMDK OneAPI","title":"Installing OAP"},{"location":"OAP-Installation-Guide/#extra-steps-for-shuffle-remote-pmem-extension","text":"If you use one of OAP features -- PMem Shuffle with RDMA , you need to configure and validate RDMA, please refer to PMem Shuffle for the details.","title":"Extra Steps for Shuffle Remote PMem Extension"},{"location":"OAP-Installation-Guide/#configuration","text":"Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf . spark.executorEnv.LD_LIBRARY_PATH $HOME/miniconda2/envs/oapenv/lib spark.executor.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.driver.extraLibraryPath $HOME/miniconda2/envs/oapenv/lib spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar Then you can follow the corresponding feature documents for more details to use them.","title":"Configuration"},{"location":"User-Guide/","text":"User Guide Prerequisites Getting Started Configuration for YARN Cluster Mode Configuration for Spark Standalone Mode Working with SQL Index Working with SQL Data Source Cache Run TPC-DS Benchmark Advanced Configuration Prerequisites SQL Index and Data Source Cache on Spark 3.1.1 requires a working Hadoop cluster with YARN and Spark. Running Spark on YARN requires a binary distribution of Spark, which is built with YARN support. Getting Started Building We have provided a Conda package which will automatically install dependencies and build OAP jars, please follow OAP-Installation-Guide and you can find compiled OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars once finished the installation. If you\u2019d like to build from source code, please refer to Developer Guide for the detailed steps. Spark Configurations Users usually test and run Spark SQL or Scala scripts in Spark Shell, which launches Spark applications on YRAN with client mode. In this section, we will start with Spark Shell then introduce other use scenarios. Before you run $SPARK_HOME/bin/spark-shell , you need to configure Spark for integration. You need to add or update the following configurations in the Spark configuration file $SPARK_HOME/conf/spark-defaults.conf on your working node. spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node spark.files $HOME/miniconda2/envs/oapenv/oap_jars/plasma-sql-ds-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/pmem-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.executor.extraClassPath ./plasma-sql-ds-cache-<version>-with-spark-<version>.jar:./pmem-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/plasma-sql-ds-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/pmem-common-<version>-with-spark-<version>.jar Verify Integration After configuration, you can follow these steps to verify the OAP integration is working using Spark Shell. Create a test data path on your HDFS. hdfs:///user/oap/ for example. hadoop fs -mkdir -p /user/oap/ Launch Spark Shell using the following command on your working node. $SPARK_HOME/bin/spark-shell Execute the following commands in Spark Shell to test OAP integration. > spark.sql(s\"\"\"CREATE TABLE oap_test (a INT, b STRING) USING parquet OPTIONS (path 'hdfs:///user/oap/')\"\"\".stripMargin) > val data = (1 to 30000).map { i => (i, s\"this is test $i\") }.toDF().createOrReplaceTempView(\"t\") > spark.sql(\"insert overwrite table oap_test select * from t\") > spark.sql(\"create oindex index1 on oap_test (a)\") > spark.sql(\"show oindex from oap_test\").show() This test creates an index for a table and then shows it. If there are no errors, the OAP .jar is working with the configuration. The picture below is an example of a successfully run. Configuration for YARN Cluster Mode Spark Shell, Spark SQL CLI and Thrift Sever run Spark application in client mode. While Spark Submit tool can run Spark application in client or cluster mode, which is decided by --deploy-mode parameter. Getting Started session has shown the configurations needed for client mode. If you are running Spark Submit tool in cluster mode, you need to follow the below configuration steps instead. Add the following OAP configuration settings to $SPARK_HOME/conf/spark-defaults.conf on your working node before running spark-submit in cluster mode. spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path on your working node spark.files $HOME/miniconda2/envs/oapenv/oap_jars/plasma-sql-ds-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/pmem-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.executor.extraClassPath ./plasma-sql-ds-cache-<version>-with-spark-<version>.jar:./pmem-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.driver.extraClassPath ./plasma-sql-ds-cache-<version>-with-spark-<version>.jar:./pmem-common-<version>-with-spark-<version>.jar Configuration for Spark Standalone Mode In addition to running on the YARN cluster manager, Spark also provides a simple standalone deploy mode. If you are using Spark in Spark Standalone mode: Make sure the OAP .jar at the same path of all the worker nodes. Add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf on the working node. spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path on worker nodes spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/plasma-sql-ds-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/pmem-common-<version>-with-spark-<version>.jar # absolute path on worker nodes spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/plasma-sql-ds-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/pmem-common-<version>-with-spark-<version>.jar Working with SQL Index After a successful OAP integration, you can use OAP SQL DDL to manage table indexes. The DDL operations include index create , drop , refresh , and show . Test these functions using the following examples in Spark Shell. > spark.sql(s\"\"\"CREATE TABLE oap_test (a INT, b STRING) USING parquet OPTIONS (path 'hdfs:///user/oap/')\"\"\".stripMargin) > val data = (1 to 30000).map { i => (i, s\"this is test $i\") }.toDF().createOrReplaceTempView(\"t\") > spark.sql(\"insert overwrite table oap_test select * from t\") Index Creation Use the CREATE OINDEX DDL command to create a B+ Tree index or bitmap index. CREATE OINDEX index_name ON table_name (column_name) USING [BTREE, BITMAP] The following example creates a B+ Tree index on column \"a\" of the oap_test table. > spark.sql(\"create oindex index1 on oap_test (a)\") Use SHOW OINDEX command to show all the created indexes on a specified table. > spark.sql(\"show oindex from oap_test\").show() NOTE: Currently SQL Index supports to create index on the following data types of columns: ByteType , ShortType , IntegerType , LongType , FloatType , DoubleType , StringType , BinaryType , BooleanType Run DESC to make sure the data type of columns, e.g. DecimalType , DateType are not supported. > spark.sql(\"desc oap_test\").show() Use Index Using index in a query is transparent. When SQL queries have filter conditions on the column(s) which can take advantage of the index to filter the data scan, the index will automatically be applied to the execution of Spark SQL. The following example will automatically use the underlayer index created on column \"a\". > spark.sql(\"SELECT * FROM oap_test WHERE a = 1\").show() NOTE: If you choose to use SQL Index to get performance gain in queries, we recommend you do not create index on small tables, like dimension tables. Drop index Use DROP OINDEX command to drop a named index. > spark.sql(\"drop oindex index1 on oap_test\") Working with SQL Data Source Cache Data Source Cache can provide input data cache functionality to the executor. When using the cache data among different SQL queries, configure cache to allow different SQL queries to use the same executor process. Do this by running your queries through the Spark ThriftServer as shown below. For cache media, we support both DRAM and Intel PMem which means you can choose to cache data in DRAM or Intel PMem if you have PMem configured in hardware. Use DRAM Cache Make the following configuration changes in Spark configuration file $SPARK_HOME/conf/spark-defaults.conf . spark.memory.offHeap.enabled false spark.oap.cache.strategy guava spark.sql.oap.cache.memory.manager offheap # according to the resource of cluster spark.executor.memoryOverhead 50g # equal to the size of executor.memoryOverhead spark.executor.sql.oap.cache.offheap.memory.size 50g # for parquet fileformat, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for orc fileformat, enable binary cache spark.sql.oap.orc.binary.cache.enabled true NOTE : Change spark.executor.sql.oap.cache.offheap.memory.size based on the availability of DRAM capacity to cache data, and its size is equal to spark.executor.memoryOverhead Launch Spark ThriftServer Launch Spark Thrift Server, and use the Beeline command line tool to connect to the Thrift Server to execute DDL or DML operations. The data cache will automatically take effect for Parquet or ORC file sources. The rest of this section will show you how to do a quick verification of cache functionality. It will reuse the database metastore created in the Working with SQL Index section, which creates the oap_test table definition. In production, Spark Thrift Server will have its own metastore database directory or metastore service and use DDL's through Beeline for creating your tables. When you run spark-shell to create the oap_test table, metastore_db will be created in the directory where you ran '$SPARK_HOME/bin/spark-shell'. Go to that directory and execute the following command to launch Thrift JDBC server and run queries. $SPARK_HOME/sbin/start-thriftserver.sh Use Beeline and connect to the Thrift JDBC server, replacing the hostname (mythriftserver) with your own Thrift Server hostname. . $SPARK_HOME/bin/beeline -u jdbc:hive2://<mythriftserver>:10000 After the connection is established, execute the following commands to check the metastore is initialized correctly. > SHOW databases; > USE default; > SHOW tables; Run queries on the table that will use the cache automatically. For example, > SELECT * FROM oap_test WHERE a = 1; > SELECT * FROM oap_test WHERE a = 2; > SELECT * FROM oap_test WHERE a = 3; ... Open the Spark History Web UI and go to the OAP tab page to verify the cache metrics. The following picture is an example. Use PMem Cache Prerequisites The following steps are required to configure OAP to use PMem cache with external cache strategy. PMem hardware is successfully deployed on each node in cluster. Besides, when enabling SQL Data Source Cache with external cache using Plasma, PMem could get noticeable performance gain with BIOS configuration settings below, especially on cross socket write path. Socket Configuration -> Memory Configuration -> NGN Configuration -> Snoopy mode for AD : Enabled Socket Configuration -> Intel UPI General Configuration -> Stale AtoS : Disabled It's strongly advised to use Linux device mapper to interleave PMem across sockets and get maximum size for Plasma. // use ipmctl command to show topology and dimm info of PMem ipmctl show -topology ipmctl show -dimm // provision PMem in app direct mode ipmctl create -goal PersistentMemoryType=AppDirect // reboot system to make configuration take affect reboot // check capacity provisioned for app direct mode(AppDirectCapacity) ipmctl show -memoryresources // show the PMem region information ipmctl show -region // create namespace based on the region, multi namespaces can be created on a single region ndctl create-namespace -m fsdax -r region0 ndctl create-namespace -m fsdax -r region1 // show the created namespaces fdisk -l // create and mount file system sudo dmsetup create striped-pmem mkfs.ext4 -b 4096 -E stride=512 -F /dev/mapper/striped-pmem mkdir -p /mnt/pmem mount -o dax /dev/mapper/striped-pmem /mnt/pmem For more information you can refer to Quick Start Guide: Provision Intel\u00ae Optane\u2122 DC Persistent Memory SQL Data Source Cache uses Plasma as a node-level external cache service, the benefit of using external cache is data could be shared across process boundaries. Plasma is a high-performance shared-memory object store and a component of Apache Arrow . We have modified Plasma to support PMem, and make it open source on oap-project-Arrow repo. If you have finished OAP Installation Guide , Plasma will be automatically installed and then you just need copy arrow-plasma-4.0.0.jar to $SPARK_HOME/jars . For manual building and installation steps you can refer to Plasma installation . Refer to configuration below to apply external cache strategy and start Plasma service on each node and start your workload. Configuration for enabling PMem cache Add the following configuration to $SPARK_HOME/conf/spark-defaults.conf . # 2x number of your worker nodes spark.executor.instances 6 # enable SQL Index and Data Source Cache extension in Spark spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node, when in Yarn client mode spark.files $HOME/miniconda2/envs/oapenv/oap_jars/plasma-sql-ds-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/pmem-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir, when in Yarn client mode spark.executor.extraClassPath ./plasma-sql-ds-cache-<version>-with-spark-<version>.jar:./pmem-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node,when in Yarn client mode spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/plasma-sql-ds-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/pmem-common-<version>-with-spark-<version>.jar # for parquet file format, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for ORC file format, enable binary cache spark.sql.oap.orc.binary.cache.enabled true # enable external cache strategy spark.oap.cache.strategy external spark.sql.oap.dcpmm.free.wait.threshold 50000000000 # according to your executor core number spark.executor.sql.oap.cache.external.client.pool.size 10 Start Plasma service manually Plasma config parameters: -m how much Bytes share memory Plasma will use -s Unix Domain sockcet path -d PMem directory Start Plasma service on each node with following command, then run your workload. If you install OAP by Conda, you can find plasma-store-server in the path $HOME/miniconda2/envs/oapenv/bin/ . Change the permission of /tmp/plasmaStore to 777 if you are not root user to run following command. ./plasma-store-server -m 15000000000 -s /tmp/plasmaStore -d /mnt/pmem Remember to kill plasma-store-server process if you no longer need cache, and you should delete /tmp/plasmaStore which is a Unix domain socket. Use Yarn to start Plasma service When using Yarn(Hadoop version >= 3.1) to start Plasma service, you should provide a json file as below. { \"name\": \"plasma-store-service\", \"version\": 1, \"components\" : [ { \"name\": \"plasma-store-service\", \"number_of_containers\": 3, \"launch_command\": \"plasma-store-server -m 15000000000 -s /tmp/plasmaStore -d /mnt/pmem\", \"resource\": { \"cpus\": 1, \"memory\": 512 } } ] } Run command yarn app -launch plasma-store-service /tmp/plasmaLaunch.json to start Plasma server. Run yarn app -stop plasma-store-service to stop it. Run yarn app -destroy plasma-store-service to destroy it. Verify PMem cache functionality After finishing configuration, restart Spark Thrift Server for the configuration changes to take effect. Start at step 2 of the Use DRAM Cache guide to verify that cache is working correctly. Check PMem cache size by checking disk space with df -h . Run TPC-DS Benchmark This section provides instructions and tools for running TPC-DS queries to evaluate the cache performance of various configurations. The TPC-DS suite has many queries and we select 9 I/O intensive queries to simplify performance evaluation. We created some tool scripts oap-benchmark-tool.zip to simplify running the workload. If you are already familiar with TPC-DS data generation and running a TPC-DS tool suite, skip our tool and use the TPC-DS tool suite directly. Prerequisites Python 2.7+ is required on the working node. Prepare the Tool Download oap-benchmark-tool.zip and unzip to a folder (for example, oap-benchmark-tool folder) on your working node. Copy oap-benchmark-tool/tools/tpcds-kits to ALL worker nodes under the same folder (for example, /home/oap/tpcds-kits ). Generate TPC-DS Data Update the values for the following variables in oap-benchmark-tool/scripts/tool.conf based on your environment and needs. SPARK_HOME: Point to the Spark home directory of your Spark setup. TPCDS_KITS_DIR: The tpcds-kits directory you coped to the worker nodes in the above prepare process. For example, /home/oap/tpcds-kits NAMENODE_ADDRESS: Your HDFS Namenode address in the format of host:port. THRIFT_SERVER_ADDRESS: Your working node address on which you will run Thrift Server. DATA_SCALE: The data scale to be generated in GB DATA_FORMAT: The data file format. You can specify parquet or orc For example: export SPARK_HOME=/home/oap/spark-3.1.1 export TPCDS_KITS_DIR=/home/oap/tpcds-kits export NAMENODE_ADDRESS=mynamenode:9000 export THRIFT_SERVER_ADDRESS=mythriftserver export DATA_SCALE=1024 export DATA_FORMAT=parquet Start data generation. In the root directory of this tool ( oap-benchmark-tool ), run scripts/run_gen_data.sh to start the data generation process. cd oap-benchmark-tool sh ./scripts/run_gen_data.sh Once finished, the $scale data will be generated in the HDFS folder genData$scale . And a database called tpcds_$format$scale will contain the TPC-DS tables. Start Spark Thrift Server Start the Thrift Server in the tool root folder, which is the same folder you run data generation scripts. Use either the PMem or DRAM script to start the Thrift Server. Use PMem as Cache Media Update the configuration values in scripts/spark_thrift_server_yarn_with_PMem.sh to reflect your environment. Normally, you need to update the following configuration values to cache to PMem. --num-executors --driver-memory --executor-memory --executor-cores --conf spark.oap.cache.strategy --conf spark.sql.oap.dcpmm.free.wait.threshold --conf spark.executor.sql.oap.cache.external.client.pool.size These settings will override the values specified in Spark configuration file ( spark-defaults.conf ). After the configuration is done, you can execute the following command to start Thrift Server. cd oap-benchmark-tool sh ./scripts/spark_thrift_server_yarn_with_PMem.sh start In this script, we use external as cache strategy for Parquet Binary data cache. Use DRAM as Cache Media Update the configuration values in scripts/spark_thrift_server_yarn_with_DRAM.sh to reflect your environment. Normally, you need to update the following configuration values to cache to DRAM. --num-executors --driver-memory --executor-memory --executor-cores --conf spark.executor.sql.oap.cache.offheap.memory.size --conf spark.executor.memoryOverhead These settings will override the values specified in Spark configuration file ( spark-defaults.conf ). After the configuration is done, you can execute the following command to start Thrift Server. cd oap-benchmark-tool sh ./scripts/spark_thrift_server_yarn_with_DRAM.sh start Run Queries Execute the following command to start to run queries. If you use external cache strategy, also need start plasma service manually as above. cd oap-benchmark-tool sh ./scripts/run_tpcds.sh When all the queries are done, you will see the result.json file in the current directory. You will find in the 2nd and 3rd round, cache feature takes effect and query time becomes less. And the Spark webUI OAP tab has more specific OAP cache metrics just as section step 5. Advanced Configuration Cache Hot Tables Data Source Cache also supports caching specific tables according to actual situations, these tables are usually hot tables. Column Vector Cache This document above uses binary cache as example for Parquet file format, if your cluster memory resources is abundant enough, you can choose ColumnVector data cache instead of binary cache for Parquet to spare computation time. Large Scale and Heterogeneous Cluster Support Introduce an external database to store cache locality info to support large-scale and heterogeneous clusters. For more information and configuration details, please refer to Advanced Configuration .","title":"User Guide"},{"location":"User-Guide/#user-guide","text":"Prerequisites Getting Started Configuration for YARN Cluster Mode Configuration for Spark Standalone Mode Working with SQL Index Working with SQL Data Source Cache Run TPC-DS Benchmark Advanced Configuration","title":"User Guide"},{"location":"User-Guide/#prerequisites","text":"SQL Index and Data Source Cache on Spark 3.1.1 requires a working Hadoop cluster with YARN and Spark. Running Spark on YARN requires a binary distribution of Spark, which is built with YARN support.","title":"Prerequisites"},{"location":"User-Guide/#getting-started","text":"","title":"Getting Started"},{"location":"User-Guide/#building","text":"We have provided a Conda package which will automatically install dependencies and build OAP jars, please follow OAP-Installation-Guide and you can find compiled OAP jars under $HOME/miniconda2/envs/oapenv/oap_jars once finished the installation. If you\u2019d like to build from source code, please refer to Developer Guide for the detailed steps.","title":"Building"},{"location":"User-Guide/#spark-configurations","text":"Users usually test and run Spark SQL or Scala scripts in Spark Shell, which launches Spark applications on YRAN with client mode. In this section, we will start with Spark Shell then introduce other use scenarios. Before you run $SPARK_HOME/bin/spark-shell , you need to configure Spark for integration. You need to add or update the following configurations in the Spark configuration file $SPARK_HOME/conf/spark-defaults.conf on your working node. spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node spark.files $HOME/miniconda2/envs/oapenv/oap_jars/plasma-sql-ds-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/pmem-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.executor.extraClassPath ./plasma-sql-ds-cache-<version>-with-spark-<version>.jar:./pmem-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/plasma-sql-ds-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/pmem-common-<version>-with-spark-<version>.jar","title":"Spark Configurations"},{"location":"User-Guide/#verify-integration","text":"After configuration, you can follow these steps to verify the OAP integration is working using Spark Shell. Create a test data path on your HDFS. hdfs:///user/oap/ for example. hadoop fs -mkdir -p /user/oap/ Launch Spark Shell using the following command on your working node. $SPARK_HOME/bin/spark-shell Execute the following commands in Spark Shell to test OAP integration. > spark.sql(s\"\"\"CREATE TABLE oap_test (a INT, b STRING) USING parquet OPTIONS (path 'hdfs:///user/oap/')\"\"\".stripMargin) > val data = (1 to 30000).map { i => (i, s\"this is test $i\") }.toDF().createOrReplaceTempView(\"t\") > spark.sql(\"insert overwrite table oap_test select * from t\") > spark.sql(\"create oindex index1 on oap_test (a)\") > spark.sql(\"show oindex from oap_test\").show() This test creates an index for a table and then shows it. If there are no errors, the OAP .jar is working with the configuration. The picture below is an example of a successfully run.","title":"Verify Integration"},{"location":"User-Guide/#configuration-for-yarn-cluster-mode","text":"Spark Shell, Spark SQL CLI and Thrift Sever run Spark application in client mode. While Spark Submit tool can run Spark application in client or cluster mode, which is decided by --deploy-mode parameter. Getting Started session has shown the configurations needed for client mode. If you are running Spark Submit tool in cluster mode, you need to follow the below configuration steps instead. Add the following OAP configuration settings to $SPARK_HOME/conf/spark-defaults.conf on your working node before running spark-submit in cluster mode. spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path on your working node spark.files $HOME/miniconda2/envs/oapenv/oap_jars/plasma-sql-ds-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/pmem-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.executor.extraClassPath ./plasma-sql-ds-cache-<version>-with-spark-<version>.jar:./pmem-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir spark.driver.extraClassPath ./plasma-sql-ds-cache-<version>-with-spark-<version>.jar:./pmem-common-<version>-with-spark-<version>.jar","title":"Configuration for YARN Cluster Mode"},{"location":"User-Guide/#configuration-for-spark-standalone-mode","text":"In addition to running on the YARN cluster manager, Spark also provides a simple standalone deploy mode. If you are using Spark in Spark Standalone mode: Make sure the OAP .jar at the same path of all the worker nodes. Add the following configuration settings to $SPARK_HOME/conf/spark-defaults.conf on the working node. spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path on worker nodes spark.executor.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/plasma-sql-ds-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/pmem-common-<version>-with-spark-<version>.jar # absolute path on worker nodes spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/plasma-sql-ds-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/pmem-common-<version>-with-spark-<version>.jar","title":"Configuration for Spark Standalone Mode"},{"location":"User-Guide/#working-with-sql-index","text":"After a successful OAP integration, you can use OAP SQL DDL to manage table indexes. The DDL operations include index create , drop , refresh , and show . Test these functions using the following examples in Spark Shell. > spark.sql(s\"\"\"CREATE TABLE oap_test (a INT, b STRING) USING parquet OPTIONS (path 'hdfs:///user/oap/')\"\"\".stripMargin) > val data = (1 to 30000).map { i => (i, s\"this is test $i\") }.toDF().createOrReplaceTempView(\"t\") > spark.sql(\"insert overwrite table oap_test select * from t\")","title":"Working with SQL Index"},{"location":"User-Guide/#index-creation","text":"Use the CREATE OINDEX DDL command to create a B+ Tree index or bitmap index. CREATE OINDEX index_name ON table_name (column_name) USING [BTREE, BITMAP] The following example creates a B+ Tree index on column \"a\" of the oap_test table. > spark.sql(\"create oindex index1 on oap_test (a)\") Use SHOW OINDEX command to show all the created indexes on a specified table. > spark.sql(\"show oindex from oap_test\").show() NOTE: Currently SQL Index supports to create index on the following data types of columns: ByteType , ShortType , IntegerType , LongType , FloatType , DoubleType , StringType , BinaryType , BooleanType Run DESC to make sure the data type of columns, e.g. DecimalType , DateType are not supported. > spark.sql(\"desc oap_test\").show()","title":"Index Creation"},{"location":"User-Guide/#use-index","text":"Using index in a query is transparent. When SQL queries have filter conditions on the column(s) which can take advantage of the index to filter the data scan, the index will automatically be applied to the execution of Spark SQL. The following example will automatically use the underlayer index created on column \"a\". > spark.sql(\"SELECT * FROM oap_test WHERE a = 1\").show() NOTE: If you choose to use SQL Index to get performance gain in queries, we recommend you do not create index on small tables, like dimension tables.","title":"Use Index"},{"location":"User-Guide/#drop-index","text":"Use DROP OINDEX command to drop a named index. > spark.sql(\"drop oindex index1 on oap_test\")","title":"Drop index"},{"location":"User-Guide/#working-with-sql-data-source-cache","text":"Data Source Cache can provide input data cache functionality to the executor. When using the cache data among different SQL queries, configure cache to allow different SQL queries to use the same executor process. Do this by running your queries through the Spark ThriftServer as shown below. For cache media, we support both DRAM and Intel PMem which means you can choose to cache data in DRAM or Intel PMem if you have PMem configured in hardware.","title":"Working with SQL Data Source Cache"},{"location":"User-Guide/#use-dram-cache","text":"Make the following configuration changes in Spark configuration file $SPARK_HOME/conf/spark-defaults.conf . spark.memory.offHeap.enabled false spark.oap.cache.strategy guava spark.sql.oap.cache.memory.manager offheap # according to the resource of cluster spark.executor.memoryOverhead 50g # equal to the size of executor.memoryOverhead spark.executor.sql.oap.cache.offheap.memory.size 50g # for parquet fileformat, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for orc fileformat, enable binary cache spark.sql.oap.orc.binary.cache.enabled true NOTE : Change spark.executor.sql.oap.cache.offheap.memory.size based on the availability of DRAM capacity to cache data, and its size is equal to spark.executor.memoryOverhead Launch Spark ThriftServer Launch Spark Thrift Server, and use the Beeline command line tool to connect to the Thrift Server to execute DDL or DML operations. The data cache will automatically take effect for Parquet or ORC file sources. The rest of this section will show you how to do a quick verification of cache functionality. It will reuse the database metastore created in the Working with SQL Index section, which creates the oap_test table definition. In production, Spark Thrift Server will have its own metastore database directory or metastore service and use DDL's through Beeline for creating your tables. When you run spark-shell to create the oap_test table, metastore_db will be created in the directory where you ran '$SPARK_HOME/bin/spark-shell'. Go to that directory and execute the following command to launch Thrift JDBC server and run queries. $SPARK_HOME/sbin/start-thriftserver.sh Use Beeline and connect to the Thrift JDBC server, replacing the hostname (mythriftserver) with your own Thrift Server hostname. . $SPARK_HOME/bin/beeline -u jdbc:hive2://<mythriftserver>:10000 After the connection is established, execute the following commands to check the metastore is initialized correctly. > SHOW databases; > USE default; > SHOW tables; Run queries on the table that will use the cache automatically. For example, > SELECT * FROM oap_test WHERE a = 1; > SELECT * FROM oap_test WHERE a = 2; > SELECT * FROM oap_test WHERE a = 3; ... Open the Spark History Web UI and go to the OAP tab page to verify the cache metrics. The following picture is an example.","title":"Use DRAM Cache"},{"location":"User-Guide/#use-pmem-cache","text":"","title":"Use PMem Cache"},{"location":"User-Guide/#prerequisites_1","text":"The following steps are required to configure OAP to use PMem cache with external cache strategy. PMem hardware is successfully deployed on each node in cluster. Besides, when enabling SQL Data Source Cache with external cache using Plasma, PMem could get noticeable performance gain with BIOS configuration settings below, especially on cross socket write path. Socket Configuration -> Memory Configuration -> NGN Configuration -> Snoopy mode for AD : Enabled Socket Configuration -> Intel UPI General Configuration -> Stale AtoS : Disabled It's strongly advised to use Linux device mapper to interleave PMem across sockets and get maximum size for Plasma. // use ipmctl command to show topology and dimm info of PMem ipmctl show -topology ipmctl show -dimm // provision PMem in app direct mode ipmctl create -goal PersistentMemoryType=AppDirect // reboot system to make configuration take affect reboot // check capacity provisioned for app direct mode(AppDirectCapacity) ipmctl show -memoryresources // show the PMem region information ipmctl show -region // create namespace based on the region, multi namespaces can be created on a single region ndctl create-namespace -m fsdax -r region0 ndctl create-namespace -m fsdax -r region1 // show the created namespaces fdisk -l // create and mount file system sudo dmsetup create striped-pmem mkfs.ext4 -b 4096 -E stride=512 -F /dev/mapper/striped-pmem mkdir -p /mnt/pmem mount -o dax /dev/mapper/striped-pmem /mnt/pmem For more information you can refer to Quick Start Guide: Provision Intel\u00ae Optane\u2122 DC Persistent Memory SQL Data Source Cache uses Plasma as a node-level external cache service, the benefit of using external cache is data could be shared across process boundaries. Plasma is a high-performance shared-memory object store and a component of Apache Arrow . We have modified Plasma to support PMem, and make it open source on oap-project-Arrow repo. If you have finished OAP Installation Guide , Plasma will be automatically installed and then you just need copy arrow-plasma-4.0.0.jar to $SPARK_HOME/jars . For manual building and installation steps you can refer to Plasma installation . Refer to configuration below to apply external cache strategy and start Plasma service on each node and start your workload.","title":"Prerequisites"},{"location":"User-Guide/#configuration-for-enabling-pmem-cache","text":"Add the following configuration to $SPARK_HOME/conf/spark-defaults.conf . # 2x number of your worker nodes spark.executor.instances 6 # enable SQL Index and Data Source Cache extension in Spark spark.sql.extensions org.apache.spark.sql.OapExtensions # absolute path of the jar on your working node, when in Yarn client mode spark.files $HOME/miniconda2/envs/oapenv/oap_jars/plasma-sql-ds-cache-<version>-with-spark-<version>.jar,$HOME/miniconda2/envs/oapenv/oap_jars/pmem-common-<version>-with-spark-<version>.jar # relative path to spark.files, just specify jar name in current dir, when in Yarn client mode spark.executor.extraClassPath ./plasma-sql-ds-cache-<version>-with-spark-<version>.jar:./pmem-common-<version>-with-spark-<version>.jar # absolute path of the jar on your working node,when in Yarn client mode spark.driver.extraClassPath $HOME/miniconda2/envs/oapenv/oap_jars/plasma-sql-ds-cache-<version>-with-spark-<version>.jar:$HOME/miniconda2/envs/oapenv/oap_jars/pmem-common-<version>-with-spark-<version>.jar # for parquet file format, enable binary cache spark.sql.oap.parquet.binary.cache.enabled true # for ORC file format, enable binary cache spark.sql.oap.orc.binary.cache.enabled true # enable external cache strategy spark.oap.cache.strategy external spark.sql.oap.dcpmm.free.wait.threshold 50000000000 # according to your executor core number spark.executor.sql.oap.cache.external.client.pool.size 10 Start Plasma service manually Plasma config parameters: -m how much Bytes share memory Plasma will use -s Unix Domain sockcet path -d PMem directory Start Plasma service on each node with following command, then run your workload. If you install OAP by Conda, you can find plasma-store-server in the path $HOME/miniconda2/envs/oapenv/bin/ . Change the permission of /tmp/plasmaStore to 777 if you are not root user to run following command. ./plasma-store-server -m 15000000000 -s /tmp/plasmaStore -d /mnt/pmem Remember to kill plasma-store-server process if you no longer need cache, and you should delete /tmp/plasmaStore which is a Unix domain socket. Use Yarn to start Plasma service When using Yarn(Hadoop version >= 3.1) to start Plasma service, you should provide a json file as below. { \"name\": \"plasma-store-service\", \"version\": 1, \"components\" : [ { \"name\": \"plasma-store-service\", \"number_of_containers\": 3, \"launch_command\": \"plasma-store-server -m 15000000000 -s /tmp/plasmaStore -d /mnt/pmem\", \"resource\": { \"cpus\": 1, \"memory\": 512 } } ] } Run command yarn app -launch plasma-store-service /tmp/plasmaLaunch.json to start Plasma server. Run yarn app -stop plasma-store-service to stop it. Run yarn app -destroy plasma-store-service to destroy it.","title":"Configuration for enabling PMem cache"},{"location":"User-Guide/#verify-pmem-cache-functionality","text":"After finishing configuration, restart Spark Thrift Server for the configuration changes to take effect. Start at step 2 of the Use DRAM Cache guide to verify that cache is working correctly. Check PMem cache size by checking disk space with df -h .","title":"Verify PMem cache functionality"},{"location":"User-Guide/#run-tpc-ds-benchmark","text":"This section provides instructions and tools for running TPC-DS queries to evaluate the cache performance of various configurations. The TPC-DS suite has many queries and we select 9 I/O intensive queries to simplify performance evaluation. We created some tool scripts oap-benchmark-tool.zip to simplify running the workload. If you are already familiar with TPC-DS data generation and running a TPC-DS tool suite, skip our tool and use the TPC-DS tool suite directly.","title":"Run TPC-DS Benchmark"},{"location":"User-Guide/#prerequisites_2","text":"Python 2.7+ is required on the working node.","title":"Prerequisites"},{"location":"User-Guide/#prepare-the-tool","text":"Download oap-benchmark-tool.zip and unzip to a folder (for example, oap-benchmark-tool folder) on your working node. Copy oap-benchmark-tool/tools/tpcds-kits to ALL worker nodes under the same folder (for example, /home/oap/tpcds-kits ).","title":"Prepare the Tool"},{"location":"User-Guide/#generate-tpc-ds-data","text":"Update the values for the following variables in oap-benchmark-tool/scripts/tool.conf based on your environment and needs. SPARK_HOME: Point to the Spark home directory of your Spark setup. TPCDS_KITS_DIR: The tpcds-kits directory you coped to the worker nodes in the above prepare process. For example, /home/oap/tpcds-kits NAMENODE_ADDRESS: Your HDFS Namenode address in the format of host:port. THRIFT_SERVER_ADDRESS: Your working node address on which you will run Thrift Server. DATA_SCALE: The data scale to be generated in GB DATA_FORMAT: The data file format. You can specify parquet or orc For example: export SPARK_HOME=/home/oap/spark-3.1.1 export TPCDS_KITS_DIR=/home/oap/tpcds-kits export NAMENODE_ADDRESS=mynamenode:9000 export THRIFT_SERVER_ADDRESS=mythriftserver export DATA_SCALE=1024 export DATA_FORMAT=parquet Start data generation. In the root directory of this tool ( oap-benchmark-tool ), run scripts/run_gen_data.sh to start the data generation process. cd oap-benchmark-tool sh ./scripts/run_gen_data.sh Once finished, the $scale data will be generated in the HDFS folder genData$scale . And a database called tpcds_$format$scale will contain the TPC-DS tables.","title":"Generate TPC-DS Data"},{"location":"User-Guide/#start-spark-thrift-server","text":"Start the Thrift Server in the tool root folder, which is the same folder you run data generation scripts. Use either the PMem or DRAM script to start the Thrift Server.","title":"Start Spark Thrift Server"},{"location":"User-Guide/#use-pmem-as-cache-media","text":"Update the configuration values in scripts/spark_thrift_server_yarn_with_PMem.sh to reflect your environment. Normally, you need to update the following configuration values to cache to PMem. --num-executors --driver-memory --executor-memory --executor-cores --conf spark.oap.cache.strategy --conf spark.sql.oap.dcpmm.free.wait.threshold --conf spark.executor.sql.oap.cache.external.client.pool.size These settings will override the values specified in Spark configuration file ( spark-defaults.conf ). After the configuration is done, you can execute the following command to start Thrift Server. cd oap-benchmark-tool sh ./scripts/spark_thrift_server_yarn_with_PMem.sh start In this script, we use external as cache strategy for Parquet Binary data cache.","title":"Use PMem as Cache Media"},{"location":"User-Guide/#use-dram-as-cache-media","text":"Update the configuration values in scripts/spark_thrift_server_yarn_with_DRAM.sh to reflect your environment. Normally, you need to update the following configuration values to cache to DRAM. --num-executors --driver-memory --executor-memory --executor-cores --conf spark.executor.sql.oap.cache.offheap.memory.size --conf spark.executor.memoryOverhead These settings will override the values specified in Spark configuration file ( spark-defaults.conf ). After the configuration is done, you can execute the following command to start Thrift Server. cd oap-benchmark-tool sh ./scripts/spark_thrift_server_yarn_with_DRAM.sh start","title":"Use DRAM as Cache Media"},{"location":"User-Guide/#run-queries","text":"Execute the following command to start to run queries. If you use external cache strategy, also need start plasma service manually as above. cd oap-benchmark-tool sh ./scripts/run_tpcds.sh When all the queries are done, you will see the result.json file in the current directory. You will find in the 2nd and 3rd round, cache feature takes effect and query time becomes less. And the Spark webUI OAP tab has more specific OAP cache metrics just as section step 5.","title":"Run Queries"},{"location":"User-Guide/#advanced-configuration","text":"Cache Hot Tables Data Source Cache also supports caching specific tables according to actual situations, these tables are usually hot tables. Column Vector Cache This document above uses binary cache as example for Parquet file format, if your cluster memory resources is abundant enough, you can choose ColumnVector data cache instead of binary cache for Parquet to spare computation time. Large Scale and Heterogeneous Cluster Support Introduce an external database to store cache locality info to support large-scale and heterogeneous clusters. For more information and configuration details, please refer to Advanced Configuration .","title":"Advanced Configuration"}]}